{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c87283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import transformer_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974f41d",
   "metadata": {},
   "source": [
    "### TokenEmbedding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30b2847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-1.3036,  1.2301,  0.0877,  0.6346, -0.5307],\n",
      "        [ 0.5070,  1.0961,  0.6329, -1.6454, -0.4972],\n",
      "        [ 1.7762,  2.5230,  0.5927,  0.2253,  0.2891]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformer_test import TokenEmbedding\n",
    "# 假设词汇表大小为10000，嵌入维度为512\n",
    "vocab_size = 10\n",
    "d_model = 5\n",
    "\n",
    "# 实例化 TokenEmbedding\n",
    "token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "# 生成输入数据，假设输入是一个包含10个词汇索引的张量\n",
    "input_indices = torch.tensor([1,1,2,3,4])\n",
    "\n",
    "# 获取嵌入后的结果\n",
    "embedded_output = token_embedding(input_indices)\n",
    "\n",
    "print(embedded_output)  # 输出嵌入后的张量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43010a",
   "metadata": {},
   "source": [
    "### PositionEmbedding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b1f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 512])\n"
     ]
    }
   ],
   "source": [
    "from transformer_test import PositionalEmbedding\n",
    "# 假设嵌入维度为512，最大序列长度为1024\n",
    "d_model = 512\n",
    "max_len = 1024\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 实例化 PositionalEmbedding\n",
    "positional_embedding = PositionalEmbedding(d_model, max_len, device)\n",
    "\n",
    "# 生成一个模拟的输入张量，假设输入是一个形状为 (batch_size, sequence_length) 的张量\n",
    "batch_size = 128\n",
    "sequence_length = 50\n",
    "x = torch.zeros(batch_size, sequence_length, dtype=torch.long, device=device)\n",
    "\n",
    "# 获取位置编码\n",
    "pos_encoding = positional_embedding(x)\n",
    "\n",
    "print(pos_encoding.shape)  # 输出位置编码的形状"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ec969c",
   "metadata": {},
   "source": [
    "### Embedding Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72047991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[输入] input_ids.shape: torch.Size([128, 50])\n",
      "\n",
      "[Token Embedding] 输出形状: torch.Size([128, 50, 512])\n",
      "[Position Embedding] 输出形状: torch.Size([50, 512])\n",
      "\n",
      "[最终嵌入] 输出形状: torch.Size([128, 50, 512])\n",
      "\n",
      "样例数据查看：\n",
      "输入 IDs: tensor([1051, 5754, 6552, 2956, 2453])\n",
      "Token 嵌入[0,0,:5]: tensor([ 0.3116, -2.2355,  0.2612,  0.1022, -0.3265])\n",
      "位置嵌入[0,:5]: tensor([0., 1., 0., 1., 0.])\n",
      "最终嵌入[0,0,:5]: tensor([ 0.3116, -1.2355,  0.2612,  1.1022, -0.3265])\n"
     ]
    }
   ],
   "source": [
    "# 测试代码：完整 Embedding 流程\n",
    "\n",
    "# 参数配置\n",
    "batch_size = 128\n",
    "max_len = 50  # 实际序列长度\n",
    "vocab_size = 8000  # 使用 dec_voc_size\n",
    "d_model = 512\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 生成模拟输入 (batch_size, sequence_length)\n",
    "input_ids = torch.randint(low=0, high=vocab_size, \n",
    "                         size=(batch_size, max_len),\n",
    "                         device=device)\n",
    "\n",
    "print(\"[输入] input_ids.shape:\", input_ids.shape)\n",
    "# 输出示例：torch.Size([128, 50])\n",
    "\n",
    "# 初始化 Embedding 层\n",
    "token_embedding = TokenEmbedding(vocab_size, d_model).to(device)\n",
    "position_embedding = PositionalEmbedding(d_model, max_len=1024, device=device)\n",
    "\n",
    "# 前向传播\n",
    "token_embeddings = token_embedding(input_ids)  # (B, L) -> (B, L, D)\n",
    "pos_embeddings = position_embedding(input_ids) # (B, L) -> (L, D)\n",
    "\n",
    "print(\"\\n[Token Embedding] 输出形状:\", token_embeddings.shape)\n",
    "# 输出：torch.Size([128, 50, 512])\n",
    "\n",
    "print(\"[Position Embedding] 输出形状:\", pos_embeddings.shape)\n",
    "# 输出：torch.Size([50, 512])\n",
    "\n",
    "# 合并嵌入（自动广播位置编码到 batch 维度）\n",
    "final_embeddings = token_embeddings + pos_embeddings.unsqueeze(0)\n",
    "print(\"\\n[最终嵌入] 输出形状:\", final_embeddings.shape)\n",
    "# 输出：torch.Size([128, 50, 512])\n",
    "\n",
    "# 可视化样例数据\n",
    "print(\"\\n样例数据查看：\")\n",
    "print(\"输入 IDs:\", input_ids[0, :5])\n",
    "print(\"Token 嵌入[0,0,:5]:\", token_embeddings[0,0,:5].data)\n",
    "print(\"位置嵌入[0,:5]:\", pos_embeddings[0,:5].data)\n",
    "print(\"最终嵌入[0,0,:5]:\", final_embeddings[0,0,:5].data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Genesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
